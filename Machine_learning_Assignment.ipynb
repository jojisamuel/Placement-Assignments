{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlLMGAHAB9ue"
   },
   "source": [
    "##                                     **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXPm5ovMCJ6h"
   },
   "source": [
    "**INTERMEDIATE QUESTIONS :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kJQmkKNCwrJ"
   },
   "source": [
    "**Q-1.** Imagine you have a dataset where you have different Instagram features\n",
    "like u sername , Caption , Hashtag , Followers , Time_Since_posted , and likes , now your task is\n",
    "to predict the number of likes and Time Since posted and the rest of the features are\n",
    "your input features. Now you have to build a model which can predict the\n",
    "number of likes and Time Since posted.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"instagram_reach.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Feature selection/engineering\n",
    "# Combine alphanumeric values in Caption and Hashtag columns\n",
    "df['Text'] = df['Caption'].astype(str) + ' ' + df['Hashtags'].astype(str)\n",
    "\n",
    "# Convert time since posted to numeric representation\n",
    "def convert_time(time_str):\n",
    "    if 'minutes' in time_str:\n",
    "        return int(time_str.split()[0])\n",
    "    elif 'hours' in time_str:\n",
    "        return int(time_str.split()[0]) * 60\n",
    "    elif 'days' in time_str:\n",
    "        return int(time_str.split()[0]) * 60 * 24\n",
    "    else:\n",
    "        return 0  # Handle cases where time is not specified or unknown\n",
    "\n",
    "df['Time_Since_Posted_Minutes'] = df['Time since posted'].map(convert_time)\n",
    "\n",
    "# Select relevant features\n",
    "features = ['Text', 'Followers']\n",
    "target_likes = 'Likes'\n",
    "target_time_since_posted = 'Time_Since_Posted_Minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_likes, y_test_likes, y_train_time, y_test_time = train_test_split(\n",
    "    df[features], df[target_likes], df[target_time_since_posted], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text feature\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_text = vectorizer.fit_transform(X_train['Text'])\n",
    "X_test_text = vectorizer.transform(X_test['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine vectorized text features with numerical features\n",
    "X_train_combined = pd.concat([pd.DataFrame(X_train_text.toarray()), X_train['Followers'].reset_index(drop=True)], axis=1)\n",
    "X_test_combined = pd.concat([pd.DataFrame(X_test_text.toarray()), X_test['Followers'].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to strings\n",
    "X_train_combined.columns = X_train_combined.columns.astype(str)\n",
    "X_test_combined.columns = X_test_combined.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training for predicting likes\n",
    "likes_model = LinearRegression()\n",
    "likes_model.fit(X_train_combined, y_train_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes Prediction:\n",
      "Mean Squared Error (MSE): 2796.1039055933584\n",
      "Mean Absolute Error (MAE): 41.64477151011242\n",
      "Time Since Posted Prediction:\n",
      "Mean Squared Error (MSE): 45809.67560112746\n",
      "Mean Absolute Error (MAE): 140.4217466223045\n"
     ]
    }
   ],
   "source": [
    "# Model training for predicting time since posted\n",
    "time_model = LinearRegression()\n",
    "time_model.fit(X_train_combined, y_train_time)\n",
    "\n",
    "# Model evaluation\n",
    "likes_predictions = likes_model.predict(X_test_combined)\n",
    "likes_mse = mean_squared_error(y_test_likes, likes_predictions)\n",
    "likes_mae = mean_absolute_error(y_test_likes, likes_predictions)\n",
    "\n",
    "time_predictions = time_model.predict(X_test_combined)\n",
    "time_mse = mean_squared_error(y_test_time, time_predictions)\n",
    "time_mae = mean_absolute_error(y_test_time, time_predictions)\n",
    "\n",
    "print(\"Likes Prediction:\")\n",
    "print(\"Mean Squared Error (MSE):\", likes_mse)\n",
    "print(\"Mean Absolute Error (MAE):\", likes_mae)\n",
    "\n",
    "print(\"Time Since Posted Prediction:\")\n",
    "print(\"Mean Squared Error (MSE):\", time_mse)\n",
    "print(\"Mean Absolute Error (MAE):\", time_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Likes: [36.79535908]\n",
      "Predicted Time Since Posted (minutes): [125.20292986]\n"
     ]
    }
   ],
   "source": [
    "#PREDICT WITH NEW DATA\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the trained models\n",
    "likes_model = LinearRegression()\n",
    "likes_model.fit(X_train_combined, y_train_likes)\n",
    "\n",
    "time_model = LinearRegression()\n",
    "time_model.fit(X_train_combined, y_train_time)\n",
    "\n",
    "# Create a DataFrame with the new values\n",
    "new_data = pd.DataFrame({\n",
    "    'Text': [\"Dont forget to TURN ON notification\"],\n",
    "    'Followers': [151]\n",
    "})\n",
    "\n",
    "# Combine alphanumeric values in Caption and Hashtag columns\n",
    "new_data['Text'] = new_data['Text'].astype(str) + ' ' + \"#lifestyle#happiness#entrepreneurs#entrepreneurlife#business#working#founder#startup#money#magazine#moneymaker#startuplife#successful#passion#inspiredaily#hardwork#hardworkpaysoff#desire\"\n",
    "\n",
    "# Convert column names to strings\n",
    "new_data.columns = new_data.columns.astype(str)\n",
    "\n",
    "# Vectorize the text feature\n",
    "new_text = vectorizer.transform(new_data['Text'])\n",
    "\n",
    "# Combine vectorized text features with numerical features\n",
    "new_data_combined = pd.concat([pd.DataFrame(new_text.toarray()), new_data['Followers'].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Convert column names to strings\n",
    "new_data_combined.columns = new_data_combined.columns.astype(str)\n",
    "\n",
    "# Make predictions\n",
    "likes_prediction = likes_model.predict(new_data_combined)\n",
    "time_prediction = time_model.predict(new_data_combined)\n",
    "\n",
    "print(\"Predicted Likes:\", likes_prediction)\n",
    "print(\"Predicted Time Since Posted (minutes):\", time_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN6EyR4YC0bU"
   },
   "source": [
    "**Q-2.** Imagine you have a dataset where you have different features like Age ,\n",
    "Gender , Height , Weight , BMI , and Blood Pressure and you have to classify the people into\n",
    "different classes like Normal , Overweight , Obesity , Underweight , and Extreme Obesity by using\n",
    "any 4 different classification algorithms. Now you have to build a model which\n",
    "can classify people into different classes.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load and Preprocess the Dataset\n",
    "data = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "X = data.drop('NObeyesdad', axis=1)\n",
    "y = data['NObeyesdad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>family_history_with_overweight</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>NObeyesdad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Normal_Weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>87.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Walking</td>\n",
       "      <td>Overweight_Level_I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>89.8</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>Overweight_Level_II</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender   Age  Height  Weight family_history_with_overweight FAVC  FCVC  \\\n",
       "0  Female  21.0    1.62    64.0                            yes   no   2.0   \n",
       "1  Female  21.0    1.52    56.0                            yes   no   3.0   \n",
       "2    Male  23.0    1.80    77.0                            yes   no   2.0   \n",
       "3    Male  27.0    1.80    87.0                             no   no   3.0   \n",
       "4    Male  22.0    1.78    89.8                             no   no   2.0   \n",
       "\n",
       "   NCP       CAEC SMOKE  CH2O  SCC  FAF  TUE        CALC  \\\n",
       "0  3.0  Sometimes    no   2.0   no  0.0  1.0          no   \n",
       "1  3.0  Sometimes   yes   3.0  yes  3.0  0.0   Sometimes   \n",
       "2  3.0  Sometimes    no   2.0   no  2.0  1.0  Frequently   \n",
       "3  3.0  Sometimes    no   2.0   no  2.0  0.0  Frequently   \n",
       "4  1.0  Sometimes    no   2.0   no  0.0  0.0   Sometimes   \n",
       "\n",
       "                  MTRANS           NObeyesdad  \n",
       "0  Public_Transportation        Normal_Weight  \n",
       "1  Public_Transportation        Normal_Weight  \n",
       "2  Public_Transportation        Normal_Weight  \n",
       "3                Walking   Overweight_Level_I  \n",
       "4  Public_Transportation  Overweight_Level_II  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.76      0.87      0.81        86\n",
      "      Normal_Weight       0.51      0.42      0.46        93\n",
      "     Obesity_Type_I       0.59      0.51      0.55       102\n",
      "    Obesity_Type_II       0.79      0.90      0.84        88\n",
      "   Obesity_Type_III       0.93      0.99      0.96        98\n",
      " Overweight_Level_I       0.50      0.49      0.49        88\n",
      "Overweight_Level_II       0.43      0.44      0.44        79\n",
      "\n",
      "           accuracy                           0.66       634\n",
      "          macro avg       0.65      0.66      0.65       634\n",
      "       weighted avg       0.65      0.66      0.65       634\n",
      "\n",
      "Model: Decision Tree\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.90      0.95      0.93        86\n",
      "      Normal_Weight       0.83      0.82      0.82        93\n",
      "     Obesity_Type_I       0.97      0.91      0.94       102\n",
      "    Obesity_Type_II       0.96      0.98      0.97        88\n",
      "   Obesity_Type_III       1.00      0.99      0.99        98\n",
      " Overweight_Level_I       0.86      0.82      0.84        88\n",
      "Overweight_Level_II       0.89      0.95      0.92        79\n",
      "\n",
      "           accuracy                           0.92       634\n",
      "          macro avg       0.91      0.92      0.92       634\n",
      "       weighted avg       0.92      0.92      0.92       634\n",
      "\n",
      "Model: Random Forest\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.98      0.94      0.96        86\n",
      "      Normal_Weight       0.80      0.92      0.86        93\n",
      "     Obesity_Type_I       0.97      0.95      0.96       102\n",
      "    Obesity_Type_II       0.97      0.98      0.97        88\n",
      "   Obesity_Type_III       1.00      0.99      0.99        98\n",
      " Overweight_Level_I       0.90      0.84      0.87        88\n",
      "Overweight_Level_II       0.97      0.92      0.95        79\n",
      "\n",
      "           accuracy                           0.94       634\n",
      "          macro avg       0.94      0.94      0.94       634\n",
      "       weighted avg       0.94      0.94      0.94       634\n",
      "\n",
      "Model: SVM\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.69      0.83      0.75        86\n",
      "      Normal_Weight       0.44      0.34      0.39        93\n",
      "     Obesity_Type_I       0.58      0.33      0.42       102\n",
      "    Obesity_Type_II       0.75      0.47      0.57        88\n",
      "   Obesity_Type_III       0.63      1.00      0.77        98\n",
      " Overweight_Level_I       0.50      0.51      0.51        88\n",
      "Overweight_Level_II       0.42      0.53      0.47        79\n",
      "\n",
      "           accuracy                           0.57       634\n",
      "          macro avg       0.57      0.57      0.55       634\n",
      "       weighted avg       0.57      0.57      0.55       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "categorical_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE','SCC','CALC', 'MTRANS']\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "#Split the Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build and Train the Models\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Support Vector Machines (SVM)\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate and Compare the Models\n",
    "models = [('Logistic Regression', logreg), ('Decision Tree', dt), ('Random Forest', rf), ('SVM', svm)]\n",
    "for name, model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcckX8jIC0e5"
   },
   "source": [
    "**Q-3.** Imagine you have a dataset where you have different categories of data, Now\n",
    "you need to find the most similar data to the given data by using any 4 different\n",
    "similarity algorithms. Now you have to build a model which can find the most similar\n",
    "data to the given data.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_json(\"News_Category_Dataset_v3.json\", lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['category', 'headline', 'short_description']]\n",
    "data['text'] = data['headline'] + ' ' + data['short_description']\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Function to find the most similar data using different similarity algorithms\n",
    "def find_similar_data(query, top_n=5):\n",
    "    # Vectorize the query\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate similarities using different algorithms\n",
    "    similarity_scores = []\n",
    "    algorithms = [cosine_similarity, euclidean_distances, manhattan_distances]\n",
    "    for algorithm in algorithms:\n",
    "        sim = algorithm(X, query_vector).flatten()\n",
    "        similarity_scores.append(sim)\n",
    "\n",
    "    # Combine similarities from different algorithms\n",
    "    similarity_scores = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "    # Find the indices of top similar data points\n",
    "    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "\n",
    "    # Return the top similar data points\n",
    "    similar_data = data.iloc[top_indices]\n",
    "\n",
    "    return similar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109802</th>\n",
       "      <td>WORLDPOST</td>\n",
       "      <td>Weekend Roundup: Laughing at God</td>\n",
       "      <td>The first principle of an open society is not ...</td>\n",
       "      <td>Weekend Roundup: Laughing at God The first pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66816</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Sunday Roundup</td>\n",
       "      <td>This week the nation watched as the #NeverTrum...</td>\n",
       "      <td>Sunday Roundup This week the nation watched as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63109</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Sunday Roundup</td>\n",
       "      <td>This week, the nation was reminded, in ways bo...</td>\n",
       "      <td>Sunday Roundup This week, the nation was remin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107893</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Sunday Roundup</td>\n",
       "      <td>This week began with \"The Horrible Call\" final...</td>\n",
       "      <td>Sunday Roundup This week began with \"The Horri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72892</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Sunday Roundup</td>\n",
       "      <td>This week the GOP debate circus pulled into Mi...</td>\n",
       "      <td>Sunday Roundup This week the GOP debate circus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                          headline  \\\n",
       "109802  WORLDPOST  Weekend Roundup: Laughing at God   \n",
       "66816    POLITICS                    Sunday Roundup   \n",
       "63109    POLITICS                    Sunday Roundup   \n",
       "107893   POLITICS                    Sunday Roundup   \n",
       "72892    POLITICS                    Sunday Roundup   \n",
       "\n",
       "                                        short_description  \\\n",
       "109802  The first principle of an open society is not ...   \n",
       "66816   This week the nation watched as the #NeverTrum...   \n",
       "63109   This week, the nation was reminded, in ways bo...   \n",
       "107893  This week began with \"The Horrible Call\" final...   \n",
       "72892   This week the GOP debate circus pulled into Mi...   \n",
       "\n",
       "                                                     text  \n",
       "109802  Weekend Roundup: Laughing at God The first pri...  \n",
       "66816   Sunday Roundup This week the nation watched as...  \n",
       "63109   Sunday Roundup This week, the nation was remin...  \n",
       "107893  Sunday Roundup This week began with \"The Horri...  \n",
       "72892   Sunday Roundup This week the GOP debate circus...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar News\n",
    "query = \"President Joe Biden and first lady Jill Biden\"\n",
    "similar_data = find_similar_data(query)\n",
    "similar_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ag2YkYwC0h1"
   },
   "source": [
    "**Q-4.** Imagine you working as a sale manager now you need to predict the Revenue\n",
    "and whether that particular revenue is on the weekend or not and find the\n",
    "Informational_Duration using the Ensemble learning algorithm\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue Accuracy: 0.8913219789132197\n",
      "Weekend Accuracy: 0.7619626926196269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('online_shoppers_intention.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "# Drop any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['Revenue', 'Weekend', 'Informational_Duration'], axis=1)\n",
    "y_revenue = df['Revenue']\n",
    "y_weekend = df['Weekend']\n",
    "y_info_duration = df['Informational_Duration']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_revenue_train, y_revenue_test, y_weekend_train, y_weekend_test, y_info_duration_train, y_info_duration_test = train_test_split(\n",
    "    X, y_revenue, y_weekend, y_info_duration, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model for revenue prediction\n",
    "revenue_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "revenue_model.fit(X_train, y_revenue_train)\n",
    "\n",
    "# Predict revenue on the test set\n",
    "revenue_predictions = revenue_model.predict(X_test)\n",
    "\n",
    "# Train the Random Forest model for weekend prediction\n",
    "weekend_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "weekend_model.fit(X_train, y_weekend_train)\n",
    "\n",
    "# Predict weekend on the test set\n",
    "weekend_predictions = weekend_model.predict(X_test)\n",
    "\n",
    "# Train the Random Forest model for informational duration prediction\n",
    "info_duration_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "info_duration_model.fit(X_train, y_info_duration_train)\n",
    "\n",
    "# Predict informational duration on the test set\n",
    "info_duration_predictions = info_duration_model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "revenue_accuracy = accuracy_score(y_revenue_test, revenue_predictions)\n",
    "weekend_accuracy = accuracy_score(y_weekend_test, weekend_predictions)\n",
    "\n",
    "print(\"Revenue Accuracy:\", revenue_accuracy)\n",
    "print(\"Weekend Accuracy:\", weekend_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs5F47UMC0jZ"
   },
   "source": [
    "**Q-5.** Uber is a taxi service provider as we know, we need to predict the high\n",
    "booking area using an Unsupervised algorithm and price for the location using a\n",
    "supervised algorithm and use some map function to display the data\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium import plugins\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Data preprocessing\n",
    "data = pd.read_csv('rideshare_kaggle.csv')  # Load the dataset\n",
    "# Perform data cleaning and preprocessing as needed\n",
    "\n",
    "# Step 2: Unsupervised algorithm for high booking areas\n",
    "# Select relevant columns for clustering\n",
    "location_data = data[['latitude', 'longitude']]\n",
    "# Perform clustering using K-means algorithm\n",
    "kmeans = KMeans(n_clusters=5)  # Set the number of clusters as desired\n",
    "clusters = kmeans.fit_predict(location_data)\n",
    "# Add cluster labels to the dataset\n",
    "data['cluster_label'] = clusters\n",
    "\n",
    "# Step 3: Supervised algorithm for price prediction\n",
    "# Select relevant features for price prediction\n",
    "features = ['latitude', 'longitude', 'distance', 'surge_multiplier', 'temperature', 'humidity']\n",
    "X = data[features]\n",
    "y = data['price']\n",
    "# Handle missing values in y\n",
    "y = y.fillna(y.mean())  # Replace NaN values with the mean of y\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tester\\AppData\\Local\\Temp\\ipykernel_8356\\1419974274.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  heat_data['predicted_price'] = model.predict(data[features])\n"
     ]
    }
   ],
   "source": [
    "map = folium.Map(location=[data['latitude'].mean(), data['longitude'].mean()], zoom_start=10)\n",
    "\n",
    "# Add markers for high booking areas\n",
    "for index, row in data.iterrows():\n",
    "    folium.Marker(location=[row['latitude'], row['longitude']], \n",
    "                  popup=f\"Cluster: {row['cluster_label']}\").add_to(map)\n",
    "\n",
    "# Add predicted prices as a heatmap\n",
    "heat_data = data[['latitude', 'longitude', 'price']]\n",
    "heat_data['predicted_price'] = model.predict(data[features])\n",
    "heat_data = heat_data.dropna(subset=['predicted_price'])\n",
    "\n",
    "heat_data = heat_data.groupby(['latitude', 'longitude'])['predicted_price'].mean().reset_index().values.tolist()\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(map)  # Add tile layer for better visualization\n",
    "folium.plugins.HeatMap(heat_data).add_to(map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "map.save('booking_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5saDW0sC0lm"
   },
   "source": [
    "**Q-6.** Imagine you have a dataset where you have predicted loan Eligibility using any\n",
    "4 different classification algorithms. Now you have to build a model which can\n",
    "predict loan Eligibility and you need to find the accuracy of the model and built-in\n",
    "docker and use some library to display that in frontend\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIVATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fosCSLF3C0ny"
   },
   "source": [
    "**Q-7.** Imagine you have a dataset where you need to predict the Genres of Music\n",
    "using\n",
    "an Unsupervised algorithm and you need to find the accuracy of the model, built-in\n",
    "docker, and use some library to display that in frontend\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Results:\n",
      "     label  predicted_label\n",
      "0    blues                3\n",
      "1    blues                0\n",
      "2    blues                4\n",
      "3    blues                4\n",
      "4    blues                1\n",
      "..     ...              ...\n",
      "195      2                0\n",
      "196      2                0\n",
      "197      2                4\n",
      "198      2                0\n",
      "199      2                0\n",
      "\n",
      "[1200 rows x 2 columns]\n",
      "Silhouette Score: 0.20448890546374038\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the music dataset\n",
    "data_1 = pd.read_csv('data.csv')\n",
    "data_2 = pd.read_csv('data_2genre.csv')\n",
    "\n",
    "data = pd.concat([data_1, data_2])\n",
    "\n",
    "\n",
    "\n",
    "# Extract the features and drop irrelevant columns\n",
    "features = data.drop(['filename', 'label'], axis=1)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=5)  # Set the desired number of clusters\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "# Get the predicted cluster labels\n",
    "predicted_labels = kmeans.labels_\n",
    "\n",
    "# Evaluate the clustering performance\n",
    "silhouette = silhouette_score(scaled_features, predicted_labels)\n",
    "\n",
    "# Add the predicted labels to the dataset\n",
    "data['predicted_label'] = predicted_labels\n",
    "\n",
    "# Display the clustering results\n",
    "print('Clustering Results:')\n",
    "print(data[['label', 'predicted_label']])\n",
    "\n",
    "# Display the silhouette score\n",
    "print('Silhouette Score:', silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5srUZl0C0qE"
   },
   "source": [
    "**Q-8.** Quora question pair similarity, you need to find the Similarity between two\n",
    "questions by mapping the words in the questions using TF-IDF, and using a supervised\n",
    "Algorithm you need to find the similarity between the questions.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnn8LuukDgGT"
   },
   "outputs": [],
   "source": [
    "UNABLE TO DOWNLOAD DATA FROM KAGGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzk0NmQvDkKM"
   },
   "source": [
    "**Q-9.** A cyber security agent wants to check the Microsoft Malware so need he came\n",
    "to you as a Machine learning Engineering with Data, You need to find the Malware\n",
    "using a supervised algorithm and you need to find the accuracy of the model.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNABLE TO DOWNLOAD DATA FROM KAGGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-10.** An Ad- Agency analyzed a dataset of online ads and used a machine learning\n",
    "model to predict whether a user would click on an ad or not.\n",
    "Dataset This is the Dataset You can use this dataset for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNABLE TO DOWNLOAD DATA FROM KAGGLE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
